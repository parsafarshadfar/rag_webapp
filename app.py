# â”€â”€â”€ Cloudâ€‘only SQLite shim â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
__import__("pysqlite3")          # âš ï¸ Comment these three lines if running locally
import sys                       # âš ï¸ Comment these three lines if running locally
sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")  # âš ï¸ Comment these if local

# â”€â”€â”€ Standard libs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os, uuid, json, tempfile, shutil, warnings
from datetime import datetime

# â”€â”€â”€ Thirdâ€‘party libs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import streamlit as st
import chromadb
from langchain_community.llms import HuggingFaceHub
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain.schema import StrOutputParser
from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

warnings.filterwarnings("ignore", category=DeprecationWarning)
chromadb.api.client.SharedSystemClient.clear_system_cache()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ PATCH for pydantic â€œclassâ€‘notâ€‘fullyâ€‘definedâ€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
"""
Some Streamlitâ€‘Cloud containers load Pydanticâ€‘v2 and LangChain in an order that
makes forwardâ€‘references inside `HuggingFaceHub` unavailable at import time.
We provide any missing symbols *then* call `model_rebuild(force=True)` so
Pydantic reâ€‘parses the model with everything present.
"""
from typing import Union  # noqa: F401

# Provide / stubâ€‘out BaseCache if LangChain didn't export it yet
try:
    from langchain_core.cache import BaseCache  # LangChain â‰¥â€¯0.2
except ImportError:
    try:
        from langchain.cache import BaseCache   # Older LangChain
    except ImportError:
        class BaseCache:  # type: ignore
            """Minimal standâ€‘in to satisfy forward refs."""
            pass

import sys as _sys
_hf_mod = _sys.modules[HuggingFaceHub.__module__]
if not hasattr(_hf_mod, "BaseCache"):
    setattr(_hf_mod, "BaseCache", BaseCache)

try:
    HuggingFaceHub.model_rebuild(force=True)
except Exception:
    pass
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# â”€â”€â”€ Streamlit page config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(
    page_title="RAG Webapp",
    layout="wide",
    page_icon="ğŸ¤–",
    initial_sidebar_state="expanded",
)

# â”€â”€â”€ Huggingâ€¯Face embedding model (BAAI/bgeâ€‘base) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
HF_TOKEN = st.secrets["API_TOKEN"]          # put your token in `.streamlit/secrets.toml`
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HF_TOKEN

embeddings = HuggingFaceInferenceAPIEmbeddings(
    api_key=HF_TOKEN, model_name="BAAI/bge-base-en-v1.5"
)

# â”€â”€â”€ Personaâ€‘specific prompt templates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PERSONA_TEMPLATES = {
    "Friendly": '''Answer the question in a warm, conversational tone based ONLY on the following context:
    {context}

    Question: {question}

    If you don't know the answer, just say that you don't know. Keep it friendly and approachable!
    ''',
    "Formal": '''Answer the question in a professional and respectful tone based ONLY on the following context:
    {context}

    Question: {question}

    If you don't know the answer, state it politely without making up any information.
    ''',
    "Technical": '''Answer the question in a technical and detailed tone based ONLY on the following context:
    {context}

    Question: {question}

    Provide accurate, inâ€‘depth answers as applicable. Do not guess if the answer is not in the context.
    ''',
    "Concise": '''Answer the question briefly and to the point based ONLY on the following context:
    {context}

    Question: {question}

    Keep responses short and straightforward. Only answer based on the context provided.
    ''',
}

# â”€â”€â”€ Sessionâ€‘level objects â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "temperature_value" not in st.session_state:
    st.session_state.temperature_value = 0.5

if "persona" not in st.session_state:
    st.session_state.persona = "Technical"

# â”€â”€â”€ Sidebar controls â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
file_upload = st.sidebar.file_uploader("Upload your PDF", type="pdf")

st.session_state.temperature_value = st.sidebar.slider(
    "LLM Model Temperature",
    min_value=0.05,
    max_value=1.0,
    value=st.session_state.temperature_value,
    step=0.05,
)
st.sidebar.write("Lower temperature â†’ answers stick more closely to your PDF content.")

st.session_state.persona = st.sidebar.selectbox(
    "Assistant tone",
    ("Friendly", "Formal", "Technical", "Concise"),
    index=("Friendly", "Formal", "Technical", "Concise").index(st.session_state.persona),
)

st.sidebar.divider()

if st.sidebar.button("Delete PDF contents from vector DB"):
    if "vector_db" in st.session_state:
        del st.session_state["vector_db"]
        st.sidebar.success("Collection deleted.")
    else:
        st.sidebar.error("No collection to delete.")

if st.session_state.chat_history:
    history_json = json.dumps(st.session_state.chat_history, indent=2)
    st.sidebar.download_button(
        label="Download Chat History",
        data=history_json,
        file_name=f"History_{datetime.now():%Y%m%d}.json",
        mime="application/json",
    )

# â”€â”€â”€ Unique session ID & Chroma collection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
session_id = st.session_state.get("session_id") or str(uuid.uuid4())
st.session_state.session_id = session_id

if "vector_db" not in st.session_state:
    st.session_state.vector_db = Chroma(
        embedding_function=embeddings,
        collection_name=session_id,
    )
retriever = st.session_state.vector_db.as_retriever()

# â”€â”€â”€ Initialise LLM FIRST (with rebuilt model) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
repo_id = "huggingfaceh4/zephyr-7b-alpha"
model_kwargs = {
    "max_new_tokens": 256,
    "repetition_penalty": 1.1,
    "temperature": st.session_state.temperature_value,
    "top_p": 0.9,
    "return_full_text": False,
}

try:
    llm = HuggingFaceHub(repo_id=repo_id, model_kwargs=model_kwargs)
except Exception as e:
    st.error(f"ğŸš« Unable to initialise Huggingâ€¯Face model: {e}")
    st.stop()

# â”€â”€â”€ Build prompt, parser, and chain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
prompt_tpl = ChatPromptTemplate.from_template(PERSONA_TEMPLATES[st.session_state.persona])
output_parser = StrOutputParser()

chain = (
    {
        "context": retriever.with_config(run_name="Docs"),
        "question": RunnablePassthrough(),
    }
    | prompt_tpl
    | llm
    | output_parser
)

# â”€â”€â”€ Main title â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("Chat with PDF")

# â”€â”€â”€ Handle PDF upload â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if file_upload:
    st.session_state.file_name = file_upload.name

    del st.session_state.vector_db
    st.session_state.vector_db = Chroma(
        embedding_function=embeddings, collection_name=session_id
    )
    retriever = st.session_state.vector_db.as_retriever()

    tmp_dir = tempfile.mkdtemp()
    pdf_path = os.path.join(tmp_dir, f"{session_id}_{file_upload.name}")
    with open(pdf_path, "wb") as f:
        f.write(file_upload.getvalue())

    docs = PyPDFLoader(pdf_path).load()
    chunks = RecursiveCharacterTextSplitter(chunk_size=2048, chunk_overlap=128).split_documents(docs)
    st.session_state.vector_db = Chroma.from_documents(chunks, embeddings, collection_name=session_id)
    shutil.rmtree(tmp_dir)

# â”€â”€â”€ Chat interface â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if file_upload:
    msg_container = st.container(height=600, border=True)

    for m in st.session_state.chat_history:
        avatar = "ğŸ¤–" if m["role"] == "assistant" else "ğŸ¤”"
        with msg_container.chat_message(m["role"], avatar=avatar):
            st.markdown(m["content"])

    if user_msg := st.chat_input("Enter a prompt hereâ€¦"):
        st.session_state.chat_history.append({"role": "user", "content": user_msg})
        msg_container.chat_message("user", avatar="ğŸ¤”").markdown(user_msg)

        with msg_container.chat_message("assistant", avatar="ğŸ¤–"):
            with st.spinner("processingâ€¦"):
                if st.session_state.vector_db:
                    answer = chain.invoke(user_msg)
                    st.markdown(answer)
                else:
                    st.warning("Please upload a PDF first.")

        st.session_state.chat_history.append({"role": "assistant", "content": answer})
        st.rerun()
else:
    st.info("â¬…ï¸â€¯Upload a PDF from the sidebar to start chatting.")
